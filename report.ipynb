{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import logging\n",
    "\n",
    "def get_vlnbert_models(config=None):\n",
    "    config_class = BertConfig\n",
    "    from src.vlnbert.vlnbert_PREVALENT import VLNBert\n",
    "    model_class = VLNBert\n",
    "    model_name_or_path = '/home/prj21/fyp/Recurrent-VLN-BERT-Isaac/pretrained_weight/pytorch_model.bin'\n",
    "    vis_config = config_class.from_pretrained('bert-base-uncased')\n",
    "    # vis_config.img_feature_dim = 2176 # Original model dim\n",
    "    vis_config.img_feature_dim = 4096\n",
    "    vis_config.img_feature_type = \"\"\n",
    "    vis_config.vl_layers = 4\n",
    "    vis_config.la_layers = 9\n",
    "    logging.set_verbosity_error()\n",
    "    visual_model = model_class.from_pretrained(model_name_or_path, config=vis_config, ignore_mismatched_sizes=True) # The mismatched visn_fc.weight -> randomly initialized values -> fine-tuned during training to adapt to the new 4096-dimensional input features.\n",
    "\n",
    "    return visual_model\n",
    "# from src.param import args\n",
    "\n",
    "def pad_instr_tokens(instr_tokens, maxlength=20):\n",
    "\n",
    "    if len(instr_tokens) <= 2: #assert len(raw_instr_tokens) > 2\n",
    "        return None\n",
    "\n",
    "    if len(instr_tokens) > maxlength - 2: # -2 for [CLS] and [SEP]\n",
    "        instr_tokens = instr_tokens[:(maxlength-2)]\n",
    "\n",
    "    instr_tokens = ['[CLS]'] + instr_tokens + ['[SEP]']\n",
    "    num_words = len(instr_tokens)  # - 1  # include [SEP]\n",
    "    instr_tokens += ['[PAD]'] * (maxlength-len(instr_tokens))\n",
    "\n",
    "    assert len(instr_tokens) == maxlength\n",
    "\n",
    "    return instr_tokens, num_words\n",
    "# RGB transform\n",
    "rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Depth transform\n",
    "def depth_transform(depth):\n",
    "    depth = np.where(np.isfinite(depth), depth, 1.94)  # Replace NaN/inf with max depth\n",
    "    depth = torch.from_numpy(depth).float()\n",
    "    depth = (depth - 1.94) / 1.43  # Standardize based on dataset stats\n",
    "    depth = depth.unsqueeze(0).unsqueeze(0)\n",
    "    depth = F.interpolate(depth, size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "    return depth\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 869 episodes. Selecting one for analysis.\n",
      "Selected episode: 2025-03-04_11-57-47_scene_QUCTc6BB5sX_episode_662\n",
      "Instruction: Enter the bedroom, and then exit on the far side of the bedroom. Walk across the hall and enter the adjacent bedroom.\n",
      "Number of steps: 42\n",
      "Ground truth actions: ['Turn right', 'Turn right', 'Turn right', 'Move forward', 'Move forward', 'Turn right', 'Move forward', 'Move forward', 'Move forward', 'Turn right', 'Move forward', 'Turn right', 'Turn right', 'Turn right', 'Move forward', 'Turn right', 'Turn right', 'Move forward', 'Move forward', 'Turn right', 'Move forward', 'Turn right', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Turn right', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Move forward', 'Turn left', 'Move forward', 'Move forward']\n",
      "Instruction tokens: ['[CLS]', 'enter', 'the', 'bedroom', ',', 'and', 'then', 'exit', 'on', 'the']... (total length: 20)\n",
      "Processing checkpoint step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint step 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint step 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint step 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint step 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint step 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint step 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prj21/miniconda3/envs/isaaclab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define checkpoint steps\n",
    "checkpoint_steps = [1000, 2000, 3000, 4000, 6000, 8000, 10000]\n",
    "\n",
    "# Define feature directory\n",
    "feature_dir = \"../VLN-Go2-Matterport\"\n",
    "\n",
    "# List available episodes\n",
    "episode_dir = os.path.join(feature_dir, \"training_data\")\n",
    "episodes = [e for e in os.listdir(episode_dir) if os.path.isdir(os.path.join(episode_dir, e))]\n",
    "print(f\"Found {len(episodes)} episodes. Selecting one for analysis.\")\n",
    "\n",
    "# Select one episode for consistency across all checkpoints\n",
    "epidx = random.randint(0, len(episodes) - 1)\n",
    "episode_name = episodes[epidx]\n",
    "episode_path = os.path.join(episode_dir, episode_name)\n",
    "print(f\"Selected episode: {episode_name}\")\n",
    "\n",
    "# Load episode data\n",
    "with open(os.path.join(episode_path, \"instructions.txt\")) as f:\n",
    "    instruction = f.read().strip()\n",
    "rgb_paths = sorted([os.path.join(episode_path, \"rgbs\", f) for f in os.listdir(os.path.join(episode_path, \"rgbs\")) if f.endswith(\".png\")])\n",
    "depth_paths = sorted([os.path.join(episode_path, \"depths\", f) for f in os.listdir(os.path.join(episode_path, \"depths\")) if f.endswith(\".npy\")])\n",
    "with open(os.path.join(episode_path, \"actions.txt\")) as f:\n",
    "    gt_actions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Instruction: {instruction}\")\n",
    "print(f\"Number of steps: {len(rgb_paths)}\")\n",
    "print(f\"Ground truth actions: {gt_actions}\")\n",
    "\n",
    "# Create data-time_report folder\n",
    "report_dir = f\"report/report_{epidx}\"\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "# Tokenize and encode instruction\n",
    "instr_tokens = tokenizer.tokenize(instruction)\n",
    "padded_tokens, _ = pad_instr_tokens(instr_tokens, 20)\n",
    "instr_encoding = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "instr_encoding = torch.tensor(instr_encoding, dtype=torch.long).unsqueeze(0).to(device)\n",
    "lang_mask = (instr_encoding != tokenizer.pad_token_id).float().to(device)\n",
    "\n",
    "print(f\"Instruction tokens: {padded_tokens[:10]}... (total length: {len(padded_tokens)})\")\n",
    "\n",
    "# Loop over checkpoint steps\n",
    "for step in checkpoint_steps:\n",
    "    print(f\"Processing checkpoint step {step}\")\n",
    "    \n",
    "    # Load model with checkpoint\n",
    "    checkpoint_path = f\"/home/prj21/fyp/Recurrent-VLN-BERT-Isaac/checkpoints/navigation_PREVALENT/2025-03-07_20-02-28/checkpoint_{step}.pt\"\n",
    "    model = get_vlnbert_models().to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "    # print(f\"Model loaded successfully for checkpoint {step}.\")\n",
    "    \n",
    "    # Reset lists for attention scores and action probabilities\n",
    "    cross_attention_scores_list = []\n",
    "    vis_self_attention_scores_list = []\n",
    "    action_probs_list = []\n",
    "    \n",
    "    # Hook functions\n",
    "    def cross_hook(module, input, output):\n",
    "        cross_attention_scores_list.append(output[1].detach().cpu())\n",
    "    \n",
    "    def vis_self_hook(module, input, output):\n",
    "        vis_self_attention_scores_list.append(output[1].detach().cpu())\n",
    "    \n",
    "    # Register hooks on the last LXRTXLayer\n",
    "    cross_handle = model.addlayer[-1].visual_attention.register_forward_hook(cross_hook)\n",
    "    vis_self_handle = model.addlayer[-1].visn_self_att.register_forward_hook(vis_self_hook)\n",
    "    # print(\"Hooks registered for attention scores.\")\n",
    "    \n",
    "    # Process the episode\n",
    "    with torch.no_grad():\n",
    "        pooled_output, lang_output = model(\"language\", input_ids=instr_encoding, lang_mask=lang_mask)\n",
    "    \n",
    "    for t in range(len(rgb_paths)):\n",
    "        rgb = rgb_transform(Image.open(rgb_paths[t])).unsqueeze(0).to(device)\n",
    "        depth = depth_transform(np.load(depth_paths[t])).unsqueeze(0).to(device)\n",
    "        vis_mask = torch.ones(1, 1).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pooled_output, action_logits, next_lang_output = model(\"visual\", lang_output, lang_mask=lang_mask, vis_mask=vis_mask, rgb=rgb, depth=depth)\n",
    "        \n",
    "        action_probs = F.softmax(action_logits, dim=-1).squeeze(0).cpu().numpy()\n",
    "        action_probs_list.append(action_probs)\n",
    "        \n",
    "        lang_output = next_lang_output\n",
    "    \n",
    "    # Remove hooks\n",
    "    cross_handle.remove()\n",
    "    vis_self_handle.remove()\n",
    "    # print(f\"Processed {len(rgb_paths)} steps for checkpoint {step}.\")\n",
    "    \n",
    "    # Process cross-attention scores (state to instruction)\n",
    "    seq_len = instr_encoding.size(1)\n",
    "    T = len(rgb_paths)\n",
    "    state_to_instr_attention = []\n",
    "    for scores in cross_attention_scores_list:\n",
    "        attn = scores.mean(dim=1).squeeze(0)  # Average over heads: (2, seq_len)\n",
    "        state_to_instr_attention.append(attn[0].numpy())  # State token attention: (seq_len,)\n",
    "    state_to_instr_attention = np.stack(state_to_instr_attention)  # (T, seq_len)\n",
    "    \n",
    "    # Process visual self-attention scores\n",
    "    vis_self_attention = []\n",
    "    for scores in vis_self_attention_scores_list:\n",
    "        attn = scores.mean(dim=1).squeeze(0)  # (2, 2)\n",
    "        vis_self_attention.append(attn[0, 1].numpy())  # State to visual attention\n",
    "    vis_self_attention = np.array(vis_self_attention)  # (T,)\n",
    "    \n",
    "    # Plot and save attention heatmap\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(state_to_instr_attention, cmap=\"YlGnBu\", \n",
    "                xticklabels=tokenizer.convert_ids_to_tokens(instr_encoding[0].cpu().numpy()),\n",
    "                yticklabels=[f\"Step {i}\" for i in range(T)],\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title(f\"State-to-Instruction Attention Over Time (Step {step})\", fontsize=16)\n",
    "    plt.xlabel(\"Instruction Tokens\", fontsize=12)\n",
    "    plt.ylabel(\"Time Steps\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(report_dir, f\"attention_heatmap_step{step}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Stack action probabilities and plot\n",
    "    action_probs = np.stack(action_probs_list)  # (T, 3)\n",
    "    actions = [\"Move forward\", \"Turn left\", \"Turn right\"]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, action in enumerate(actions):\n",
    "        plt.plot(action_probs[:, i], label=action, marker='o')\n",
    "    plt.title(f\"Action Probabilities Over Time (Step {step})\", fontsize=16)\n",
    "    plt.xlabel(\"Time Steps\", fontsize=12)\n",
    "    plt.ylabel(\"Probability\", fontsize=12)\n",
    "    plt.legend(title=\"Actions\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    # if gt_actions:\n",
    "    #     for t in range(min(len(gt_actions), len(rgb_paths))):\n",
    "    #         plt.text(t, 0.04, gt_actions[t], rotation=45, ha='center', color='black')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(report_dir, f\"action_probs_step{step}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot and save state-to-visual attention\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(vis_self_attention, label=\"State to Visual Attention\", marker='o', color='purple')\n",
    "    plt.title(f\"State-to-Visual Self-Attention Over Time (Step {step})\", fontsize=16)\n",
    "    plt.xlabel(\"Time Steps\", fontsize=12)\n",
    "    plt.ylabel(\"Attention Weight\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(report_dir, f\"vis_self_attention_step{step}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Generate markdown report\n",
    "md_content = \"# Data-Time Report\\n\\n\"\n",
    "md_content += \"## Episode Details\\n\\n\"\n",
    "md_content += f\"- **Episode:** {episode_name}\\n\"\n",
    "md_content += f\"- **Instruction:** {instruction}\\n\"\n",
    "md_content += f\"- **Ground Truth Actions:** {', '.join(gt_actions)}\\n\\n\"\n",
    "\n",
    "for step in checkpoint_steps:\n",
    "    md_content += f\"## Checkpoint {step}\\n\\n\"\n",
    "    md_content += \"<table style=\\\"table-layout: fixed; width: 100%;\\\">\\n\"\n",
    "    md_content += \"  <tr>\\n\"\n",
    "    md_content += f\"    <td><img src=\\\"attention_heatmap_step{step}.png\\\" alt=\\\"Attention Heatmap\\\" style=\\\"width:100%;\\\"/></td>\\n\"\n",
    "    md_content += f\"    <td><img src=\\\"action_probs_step{step}.png\\\" alt=\\\"Action Probabilities\\\" style=\\\"width:100%;\\\"/></td>\\n\"\n",
    "    md_content += f\"    <td><img src=\\\"vis_self_attention_step{step}.png\\\" alt=\\\"Visual Self-Attention\\\" style=\\\"width:100%;\\\"/></td>\\n\"\n",
    "    md_content += \"  </tr>\\n\"\n",
    "    md_content += \"</table>\\n\\n\"\n",
    "\n",
    "with open(os.path.join(report_dir, \"report.md\"), \"w\") as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(\"Report generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
